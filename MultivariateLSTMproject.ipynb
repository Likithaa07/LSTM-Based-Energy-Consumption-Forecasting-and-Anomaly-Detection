{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multivariate LSTM-Based Energy Consumption Forecasting and Anomaly Detection\n",
        "\n",
        "## **Aim**\n",
        "The aim of this project is to develop a machine learning system that:\n",
        "- Accurately forecasts energy consumption in smart grids using historical usage and environmental data.\n",
        "- Detects anomalies by comparing real-time consumption data with model predictions.\n",
        "- Provides actionable insights in real-time through an interactive dashboard for energy managers and stakeholders.\n",
        "\n",
        "## **Objectives**\n",
        "1. Collect and preprocess multivariate time-series energy usage data.\n",
        "2. Train a Long Short-Term Memory (LSTM) model to predict short-term and medium-term energy consumption.\n",
        "3. Implement anomaly detection logic to flag abnormal consumption patterns.\n",
        "4. Visualize forecasts and anomalies interactively using Streamlit.\n",
        "5. Support real-time monitoring to enable preventive measures and energy optimization.\n",
        "\n",
        "## **Technologies**\n",
        "- **Python**: Core programming language.\n",
        "- **TensorFlow / Keras**: For building and training the LSTM model.\n",
        "- **Streamlit**: For dashboard development and interactivity.\n",
        "- **Pandas / NumPy / Scikit-learn**: For data preprocessing.\n",
        "- **Matplotlib / Plotly**: For visualizations.\n",
        "- **Kaggle Energy Datasets**: For historical and environmental data."
      ],
      "metadata": {
        "id": "L8ZV1KUdZTvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 – Data Acquisition & Preprocessing\n",
        "\n",
        "Acquire multivariate energy consumption dataset from Kaggle or other sources.  \n",
        "Clean, transform, and normalize data; create time-based features and split into training, validation, and test sets."
      ],
      "metadata": {
        "id": "QHjubKiVZVyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/AEP_hourly.csv\", parse_dates=['Datetime'])\n",
        "\n",
        "# Preview data\n",
        "print(\"First rows:\\n\", df.head())\n",
        "print(\"\\nInfo:\")\n",
        "print(df.info())\n",
        "\n",
        "# Check and handle missing values\n",
        "print(\"\\nMissing values per column:\\n\", df.isnull().sum())\n",
        "df = df.dropna()  # Drop rows with missing values (or impute if needed)\n",
        "\n",
        "# Feature engineering: extract time-based features\n",
        "df['hour'] = df['Datetime'].dt.hour\n",
        "df['dayofweek'] = df['Datetime'].dt.dayofweek\n",
        "df['month'] = df['Datetime'].dt.month\n",
        "\n",
        "# Normalize features except timestamp\n",
        "feature_cols = ['AEP_MW', 'hour', 'dayofweek', 'month']\n",
        "scaler = MinMaxScaler()\n",
        "df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
        "\n",
        "# Train / validation / test split\n",
        "train_size = int(len(df) * 0.7)\n",
        "val_size = int(len(df) * 0.15)\n",
        "\n",
        "train_df = df.iloc[:train_size]\n",
        "val_df = df.iloc[train_size:train_size + val_size]\n",
        "test_df = df.iloc[train_size + val_size:]\n",
        "\n",
        "print(f\"Train shape: {train_df.shape}\")\n",
        "print(f\"Validation shape: {val_df.shape}\")\n",
        "print(f\"Test shape: {test_df.shape}\")"
      ],
      "metadata": {
        "id": "UhM3YQGjZUjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 121,273 hourly records of energy consumption (AEP_MW) from 2004 to 2018, with no missing values.\n",
        "After preprocessing and feature extraction, it was split into train (70%), validation (15%), and test (15%) sets."
      ],
      "metadata": {
        "id": "haQLDAhTZ8cH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 – Model Development (LSTM)\n",
        "\n",
        "Prepare sequential data for LSTM by creating sliding windows of past observations as input and the next time step as output.  \n",
        "Build and train a multivariate LSTM model to forecast future energy consumption values."
      ],
      "metadata": {
        "id": "GrwxRoxjaQBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Sequence preparation function\n",
        "def create_sequences(data, target_column, window_size):\n",
        "    \"\"\"\n",
        "    Creates input-output sequences for LSTM.\n",
        "    data: DataFrame containing features\n",
        "    target_column: column name for prediction target\n",
        "    window_size: number of past time steps to look back\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - window_size):\n",
        "        # Select window_size rows of features\n",
        "        X.append(data.iloc[i:i+window_size].values)\n",
        "        # Select target value at the next time step\n",
        "        y.append(data.iloc[i+window_size][target_column])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Parameters\n",
        "window_size = 24  # 24 hours of past data as input\n",
        "target_col = 'AEP_MW'\n",
        "\n",
        "# Prepare sequences for train/val/test\n",
        "X_train, y_train = create_sequences(train_df[['AEP_MW', 'hour', 'dayofweek', 'month']], target_col, window_size)\n",
        "X_val, y_val = create_sequences(val_df[['AEP_MW', 'hour', 'dayofweek', 'month']], target_col, window_size)\n",
        "X_test, y_test = create_sequences(test_df[['AEP_MW', 'hour', 'dayofweek', 'month']], target_col, window_size)\n",
        "\n",
        "print(f\"Train sequences shape: {X_train.shape} -> (samples, timesteps, features)\")\n",
        "print(f\"Validation sequences shape: {X_val.shape}\")\n",
        "print(f\"Test sequences shape: {X_test.shape}\")\n",
        "\n",
        "# LSTM Model Architecture\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, activation='tanh', input_shape=(window_size, X_train.shape[2])))\n",
        "model.add(Dropout(0.2))  # Dropout for regularization\n",
        "model.add(Dense(1))      # Output layer predicts a single future value\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train Model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=10,            # Adjust as needed\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate Model\n",
        "test_loss = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss (MSE): {test_loss}\")"
      ],
      "metadata": {
        "id": "q65bNS25Zltx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM model trained on 24-hour input windows achieved steady loss reduction over 10 epochs.  \n",
        "Final test Mean Squared Error reached **2.71×10⁻⁴**, indicating high accuracy in forecasting hourly energy consumption."
      ],
      "metadata": {
        "id": "edClUEhBcKLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 – Forecasting & Anomaly Detection\n",
        "\n",
        "Use the trained LSTM model to generate energy consumption forecasts on new data.  \n",
        "Compare actual values with predictions to detect anomalies where prediction errors exceed a predefined threshold, enabling early identification of irregular energy usage patterns."
      ],
      "metadata": {
        "id": "3SZjQzxycWgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate absolute errors\n",
        "errors = np.abs(y_test - y_pred.flatten())\n",
        "\n",
        "# Define anomaly detection threshold (e.g., mean + 3 standard deviations of training errors)\n",
        "train_pred = model.predict(X_train)\n",
        "train_errors = np.abs(y_train - train_pred.flatten())\n",
        "threshold = train_errors.mean() + 3 * train_errors.std()\n",
        "\n",
        "# Detect anomalies where error exceeds the threshold\n",
        "anomalies = errors > threshold\n",
        "\n",
        "print(f\"Anomaly detection threshold: {threshold:.5f}\")\n",
        "print(f\"Number of anomalies detected: {np.sum(anomalies)}\")\n",
        "\n",
        "# Plot some examples of actual vs predicted with anomalies highlighted\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(y_test[:500], label='Actual')\n",
        "plt.plot(y_pred[:500], label='Predicted')\n",
        "plt.scatter(np.where(anomalies[:500])[0], y_test[:500][anomalies[:500]], color='red', label='Anomaly', marker='x')\n",
        "plt.title('Forecast vs Actual with Anomalies Highlighted (Sample)')\n",
        "plt.xlabel('Time step')\n",
        "plt.ylabel('Normalized Energy Consumption')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4VL39glpaZf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anomaly detection process identified **263** instances where prediction errors exceeded the threshold of **0.04937**, indicating unusual deviations between actual and forecasted energy consumption values."
      ],
      "metadata": {
        "id": "1IY6ghLWc7X6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5 – Visualization with Streamlit\n",
        "\n",
        "Implement an interactive Streamlit dashboard to visualize historical energy consumption, LSTM forecasts, and detected anomalies.  \n",
        "Enable users to explore time series data dynamically, view forecast accuracy, and identify anomaly events clearly for actionable insights."
      ],
      "metadata": {
        "id": "PFAK2a62dCKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"vis_df.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vis_df, f)"
      ],
      "metadata": {
        "id": "uHaRuAzJiiE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "81XHhelmdMuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming previous variables: df, y_test, y_pred, anomalies, window_size, scaler\n",
        "\n",
        "# Prepare a DataFrame for visualization\n",
        "# Align prediction results with original timestamps for the test set\n",
        "test_df_reset = test_df.reset_index(drop=True)\n",
        "# Due to window_size offset, slice the test_df accordingly for alignment with predictions\n",
        "vis_df = test_df_reset.iloc[window_size:].copy()\n",
        "vis_df['Actual'] = y_test\n",
        "vis_df['Predicted'] = y_pred.flatten()\n",
        "vis_df['Anomaly'] = anomalies\n",
        "\n",
        "# Inverse transform AEP_MW feature for actual and predicted values to original scale if needed\n",
        "# Here only showing normalized values, can invert if scaler fitted with original scale\n",
        "# Uncomment below if wanting original scale\n",
        "# vis_df['Actual'] = scaler.inverse_transform(vis_df[['AEP_MW']].values)\n",
        "# vis_df['Predicted'] = scaler.inverse_transform(vis_df[['Predicted']].values.reshape(-1,1))\n",
        "\n",
        "# Streamlit app\n",
        "st.title(\"Energy Consumption Forecasting and Anomaly Detection\")\n",
        "\n",
        "st.write(\"\"\"\n",
        "Interactive visualization of historical energy usage, predicted consumption, and detected anomalies.\n",
        "Zoom in on specific time frames and observe consumption dynamics with highlights on anomalies.\n",
        "\"\"\")\n",
        "\n",
        "# Select number of points to display\n",
        "num_points = st.slider(\"Select number of data points to display\", min_value=100, max_value=len(vis_df), value=500)\n",
        "\n",
        "plot_data = vis_df.head(num_points)\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "ax.plot(plot_data['Datetime'], plot_data['Actual'], label='Actual')\n",
        "ax.plot(plot_data['Datetime'], plot_data['Predicted'], label='Predicted')\n",
        "\n",
        "# Highlight anomalies\n",
        "anomaly_points = plot_data[plot_data['Anomaly']]\n",
        "\n",
        "ax.scatter(anomaly_points['Datetime'], anomaly_points['Actual'], color='red', label='Anomalies', marker='x')\n",
        "\n",
        "ax.set_xlabel(\"Datetime\")\n",
        "ax.set_ylabel(\"Normalized Energy Consumption\")\n",
        "ax.set_title(\"Energy Consumption Forecast vs Actual with Anomalies\")\n",
        "ax.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "\n",
        "st.pyplot(fig)"
      ],
      "metadata": {
        "id": "aPUXmTR-cabm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok"
      ],
      "metadata": {
        "id": "3yYbW5WTdFfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"31Dxli0Ud8ZesMryaeVOZh0aCuX_33hN9DVQbL8faiX5zazqh\")"
      ],
      "metadata": {
        "id": "Ry4AxLH0eiu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "st.title(\"Energy Consumption Forecasting and Anomaly Detection\")\n",
        "st.write(\"App content goes here...\")"
      ],
      "metadata": {
        "id": "3L9t-TyAe1Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Title & description\n",
        "st.title(\"Energy Consumption Forecasting and Anomaly Detection\")\n",
        "st.write(\"\"\"\n",
        "Interactive dashboard showcasing historical hourly energy usage,\n",
        "LSTM-based forecasts, and detected anomalies.\n",
        "\n",
        "Enables visual comparison of actual vs. predicted demand patterns\n",
        "and highlights unusual consumption events for timely insights.\n",
        "\"\"\")\n",
        "\n",
        "# Example: load prepared visualization data (replace with actual vis_df DataFrame)\n",
        "# Expecting columns: Datetime, Actual, Predicted, Anomaly\n",
        "import pickle\n",
        "with open(\"vis_df.pkl\", \"rb\") as f:\n",
        "    vis_df = pickle.load(f)\n",
        "\n",
        "# Slider to choose number of points\n",
        "num_points = st.slider(\"Select number of data points\", 100, len(vis_df), 500)\n",
        "plot_data = vis_df.head(num_points)\n",
        "\n",
        "# Plot data\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "ax.plot(plot_data['Datetime'], plot_data['Actual'], label='Actual')\n",
        "ax.plot(plot_data['Datetime'], plot_data['Predicted'], label='Predicted')\n",
        "anomaly_points = plot_data[plot_data['Anomaly']]\n",
        "ax.scatter(anomaly_points['Datetime'], anomaly_points['Actual'], color='red', label='Anomalies', marker='x')\n",
        "\n",
        "ax.set_xlabel(\"Datetime\")\n",
        "ax.set_ylabel(\"Normalized Energy Consumption\")\n",
        "ax.set_title(\"Energy Consumption Forecast vs Actual with Anomalies\")\n",
        "ax.legend()\n",
        "plt.xticks(rotation=45)\n",
        "st.pyplot(fig)"
      ],
      "metadata": {
        "id": "3vmpYLpUgU0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "print(public_url)"
      ],
      "metadata": {
        "id": "9rgCWMAbe4An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The live Streamlit dashboard is now running successfully at the ngrok URL.  \n",
        "It shows the title, description, and an interactive slider (\"Select number of data points\") that controls how many points are displayed in the actual vs. predicted energy consumption graph.  \n",
        "The graph includes red markers highlighting anomalies, enabling clear visual comparison between normal and unusual consumption patterns."
      ],
      "metadata": {
        "id": "2l3jIGJWjOqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6 – Model Evaluation & Export\n",
        "\n",
        "Assess forecasting accuracy using metrics such as RMSE, MAE, and R².  \n",
        "Export the trained LSTM model and anomaly detection results for future use or integration into other systems."
      ],
      "metadata": {
        "id": "zkZTAg6sjcQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Forecasts from your model: y_pred, actual: y_test\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"RMSE: {rmse:.6f}\")\n",
        "print(f\"MAE: {mae:.6f}\")\n",
        "print(f\"R²: {r2:.4f}\")"
      ],
      "metadata": {
        "id": "fCEyApj7jPeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"energy_forecast_model.h5\")\n",
        "print(\"✅ Model saved as energy_forecast_model.h5\")"
      ],
      "metadata": {
        "id": "cZaU3jkEj0vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "anomalies_df = vis_df.copy()\n",
        "anomalies_df.to_csv(\"anomalies_data.csv\", index=False)\n",
        "print(\"✅ Anomaly data saved as anomalies_data.csv\")"
      ],
      "metadata": {
        "id": "5M8Xxbuqj3y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "id": "YVQw5mkxkBtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "\n",
        "loaded_model = load_model(\"energy_forecast_model.h5\", custom_objects={\"mse\": MeanSquaredError()})"
      ],
      "metadata": {
        "id": "rKy5sJ4Nj6hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the loaded model on the test data\n",
        "results = loaded_model.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "# If results is a float, just print it directly\n",
        "if isinstance(results, float):\n",
        "    print(f\"Test Loss: {results}\")\n",
        "else:\n",
        "    # If multiple values (list/tuple), print them all\n",
        "    print(f\"Test Loss: {results[0]}\")\n",
        "    if len(results) > 1:\n",
        "        for i, metric_name in enumerate(loaded_model.metrics_names[1:], start=1):\n",
        "            print(f\"{metric_name}: {results[i]}\")"
      ],
      "metadata": {
        "id": "DYpIOxH2l2L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test Loss: {results}\")"
      ],
      "metadata": {
        "id": "e_n54Nbml7Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "rmse = np.sqrt(0.00032631406793370843)\n",
        "print(rmse)  # ≈ 0.01806"
      ],
      "metadata": {
        "id": "blF9dkPWl8Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "\n",
        "# Assume rmse is your calculated RMSE value\n",
        "rmse = 0.018064165298560253\n",
        "\n",
        "# Display RMSE as a metric\n",
        "st.metric(label=\"Model RMSE\", value=f\"{rmse:.4f}\")"
      ],
      "metadata": {
        "id": "Ls86nY5tmL2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LSTM forecasting model achieves an RMSE of 0.018 (≈1.8% error on normalized scale), indicating highly accurate predictions of hourly energy consumption. The forecast curve closely follows actual usage patterns, while red markers highlight anomalies that represent unusual consumption events. These anomalies may be linked to operational changes, equipment malfunctions, or abnormal demand surges, and should be reviewed for potential action."
      ],
      "metadata": {
        "id": "ZymrdL5ynPud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After plotting your forecast and anomalies in Streamlit\n",
        "st.write(\"\"\"\n",
        "**Data Interpretation:**\n",
        "The LSTM forecasting model achieves an RMSE of **0.018** (≈1.8% error on normalized scale), indicating highly accurate predictions of hourly energy consumption.\n",
        "The forecast curve closely follows actual usage patterns, while red markers highlight anomalies representing unusual consumption events.\n",
        "These anomalies may be linked to operational changes, equipment malfunctions, or abnormal demand surges, and should be reviewed for potential action.\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "X3yeZH9fmddg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "This project successfully developed and deployed an **LSTM (Long Short-Term Memory) neural network model** for energy consumption forecasting. The LSTM model achieved an RMSE of approximately **0.018** (about 1.8% error on normalized data), indicating high predictive accuracy in capturing hourly energy consumption patterns.\n",
        "\n",
        "LSTM architecture was selected due to its ability to process **sequential, time-series data** and retain information over long time dependencies. This capability makes it ideal for modeling energy usage trends influenced by daily and seasonal cycles. In contrast to traditional machine learning models, LSTM handles non-linear relationships and complex temporal dependencies, improving both forecast accuracy and anomaly detection capabilities.\n",
        "\n",
        "The Streamlit dashboard integrates live forecast visualization, anomaly indicators, and performance metrics to provide actionable insights for energy management. This approach enables efficient monitoring and early detection of abnormal consumption events, supporting proactive operational decisions.\n",
        "\n",
        "Overall, the project demonstrates the effectiveness of **deep learning with LSTM** in delivering accurate energy consumption forecasts while supporting anomaly detection for improved operational intelligence."
      ],
      "metadata": {
        "id": "Ywo_FGYOoKFF"
      }
    }
  ]
}